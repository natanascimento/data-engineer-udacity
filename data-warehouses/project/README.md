# AWS Redshift Data Warehouse

## Project Summary

The objective of this project is to create a data warehouse in the cloud for a fictional music streaming service called Sparkify. Sparkify has grown their user base and song database and want to move their processes and data onto the cloud. Their data resides in Amazon S3 storage, in directories containing JSON logs on user activity and metadata on songs.

As the data engineer assigned to the project, I have created a cluster on Redshift and built an ETL pipeline to populate it using the AWS SDK for Python. The ETL process extracts the data from S3, stages it in Redshift, then transforms the data into a set of dimensional tables in a star schema. To improve processing speeds for query analysis, I specified a distribution strategy for partitioning the tables across Redshift nodes using dist keys and ordering the data using sort keys.

## How to Run

Requirements: Configuration file with login details for an active AWS Redshift cluster and ARN for an IAM role with S3 read access.

1. Fill dwh.cfg with all valid information
2. Create a python environment 
```
virtualenv .venv
```
3. Join in the python environment
```
source .venv/bin/activate
```
3. Install all dependencies listed on requirements.txt
```
pip install -r requirements.txt
```
4. Run create_tables.py from terminal or python console to create staging and analytical tables.
5. Run etl.py from terminal or python console to process and load data into data warehouse.

## Description of Data

The Sparkify database consists of five tables in the star schema shown below. The fact table is called `songplays`, and contains a record of each songplay event generated by users of the music streaming app. There are four dimension tables. They store largely normalized data on users, artists, songs and timestamps.

## Optimization of Table Design

Redshift automatically partitions and stores database tables on multiple slices within the cluster.
* Advantages: rapid and flexible scaling
* Disadvantages: decreased query performance

Executing queries across different slices can increase copying and processing costs compared to an environment where all the data is located on a single machine.

## Data Analysis

For analyse all tables we can be use these queries:

```
SELECT COUNT(*) FROM staging_events
```

```
SELECT COUNT(*) FROM staging_songs
```

```
SELECT COUNT(*) FROM songplays
```

```
SELECT COUNT(*) FROM users
```

```
SELECT COUNT(*) FROM songs
```

```
SELECT COUNT(*) FROM artists
```

```
SELECT COUNT(*) FROM time
```

## Observations 

I've created a method to manage all configs using like a "class proxy", to get the data from the ```dwh.cfg```. But I didn't use it because I validated that there was a cleaner method to collect the .cfg data